{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229e16f8-f23c-4df9-af1b-c94681e72041",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; color: #007bff; font-size: 2em;\">\n",
    "    üìò‚úèÔ∏è Create a <span style=\"color: #ff5733;\">Linear Regression</span> Model from Scratch üöÄüìä\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d762ff-963f-4be1-a7bc-d9db2ce81f4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Content\n",
    "1. [Analysing what the Linear Regrresion model does](#1)\n",
    "2. [Estimating the Loss](#2)\n",
    "3. [Minimizing the Cost Function $J(w, b)$](#3)\n",
    "4. [Gradient Descent and Finding the Best Fit](#4)\n",
    "5. [Crating the Model](#5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "60c76794-be97-4bab-a4bb-43ff0800e15d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6a641a1d-543f-4760-9afe-a56a90a0cc06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a9174-af0a-41e0-8c3c-5676cb781239",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='1'></a>\n",
    "### Analysing what the Linear Regrresion model does\n",
    "\n",
    "We can think of this model as a neural network with one layer and one neuron, without an activation function. \n",
    "___\n",
    "\n",
    "Now, let's take a neural network with its parameters already tuned and analyze what it does:\n",
    "\n",
    "1. Takes an input matrix of the data with dimensions $\\displaystyle (m, n_x) $.\n",
    "2. Outputs an array for the prediction of a certain feature.\n",
    "3. The prediction is generated by creating a linear graph that emulates the patterns in the data as accurately as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28f2b1-3e3f-41c8-abbe-808212252491",
   "metadata": {},
   "source": [
    "lets make a function that generates the $w$ and $b$ parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2516cf75-186b-4c13-86b2-a36f349a19ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_params(dim):\n",
    "    ''' Takes as input the amount of fetures in the training set (n_x)'''\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0.0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b8554e-64e3-485f-8eb6-75d753e125cd",
   "metadata": {},
   "source": [
    "So lets make a function that takes as input $X, w, b$ and output the prediction ($\\hat{y} = wx + b$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a67811-21e2-4f07-ac98-05048b85c3fb",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} = \n",
    "\\begin{pmatrix} \n",
    "    w_1 \\\\ \n",
    "    \\vdots\\\\\n",
    "    \\vdots\\\\\n",
    "    w_{n_x} \n",
    "\\end{pmatrix}^T\n",
    "\\cdot \\space\n",
    "\\begin{pmatrix} \n",
    "    x_{1,1} & \\vdots & \\vdots & \\vdots & x_{1,n_x}\\\\\n",
    "       \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "    x_{m, 1} & \\vdots & \\vdots & \\vdots & x_{m, n_x}\\\\\n",
    "\\end{pmatrix}\n",
    "+ b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "068ee7ec-fad5-4adf-bb41-6803c543e963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    return np.dot(w.T, X) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26876ee-543f-4bc1-8506-dcf970a442d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='2'></a>\n",
    "### Estimating the Loss\n",
    "now that we know how to predict data we need a way to train the model to find the optimal values for $w$ and $b$\n",
    "* so how could we check what line fits the data best?\n",
    "for knowing which is the best line we could check how far away is from evry datapoint in the training set. or in other words the difference between $y$ and $\\hat{y}$.\n",
    "* $y$ is a vector of all the true values and $\\hat{y}$ is a vector of all of the predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea41ff-1611-47ad-b765-736f4e96f25d",
   "metadata": {},
   "source": [
    "the formula for this will be taking the L2 norm of $\\displaystyle||y - \\hat{y}||_2$ which is $\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43a7807b-a9ea-4bbb-b7f5-12fe9db64a98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mse_cost(y, y_hat):\n",
    "    ''' Mean Squared Error (MSE) cost function '''\n",
    "    return np.mean((y - y_hat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1de2ba-4ac4-4013-ae1c-add89fc2fe61",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='3'></a>\n",
    "### Minimizing the Cost Function $J(w, b)$\n",
    "now we have a function that claculates the fit of a certaine line, the next step is to minimize the cost function to find the optimal valus for $w$ and $b$.\n",
    "to find the minimun value for each parameter we take the derivative of it with respect to the cost function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302cfed4-3a65-42d8-927d-272dbacc15d8",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 4px solid #007bff; padding: 10px; background-color: #cce5ff; color: #004085;\">\n",
    "üìò <b>Note:</b> the intuition for this is that when $\\frac{\\partial{J}}{\\partial{b}}, \\frac{\\partial{J}}{\\partial{w}}$ $\\to{0}$ you have found the best values for $w$ and $b$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a4048f-1ba4-4a4d-b565-545ab0b79d1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "so lets take the derivative of each parameter with respect to the cost function. this will yeild a metric of estimating when is the best fit.\n",
    "$\\displaystyle J(w, b) = \\frac{1}{m}\\sum\\left(y - (w^Tx + b)\\right)^2$\n",
    "\n",
    "$\\displaystyle\\frac{\\partial{J}}{\\partial{w}} = \\frac{1}{m}\\sum2(y - (w^Tx +b))^{2-1}\\frac{\\partial}{\\partial{w}}(y - (w^Tx + b)) \\to \\frac{1}{m}\\sum2(y - (w^Tx + b))(-x) \\to \\color{#cce5ff}{\\frac{-2}{m} \\sum{x(y - w^Tx - b))}}$\n",
    "\n",
    "$\\displaystyle\\frac{\\partial{J}}{\\partial{b}} = \\frac{1}{m}\\sum2(y - (w^Tx + b)^{2-1}  \\frac{\\partial}{\\partial{b}}(y - (w^Tx + b) \\to \\frac{1}{m}\\sum-2(y - (w^Tx + b)) \\to \\color{#cce5ff}{\\frac{-2}{m}\\sum(y - w^Tx - b))}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11218c4a-fa5b-4d22-a02f-a77db8d94ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gradiants(w, b, X, y):\n",
    "    ''' returns a dictionary of dw, db and the mse cost '''\n",
    "    m = len(y)\n",
    "    y_hat = np.dot(w.T, X) + b\n",
    "    \n",
    "    dw = (-2/m) * np.sum(X*(y - y_hat))\n",
    "    db = (-2/m) * np.sum((y - y_hat))\n",
    "                         \n",
    "    gards = {'dw': dw, \n",
    "             'db': db}\n",
    "    cost = mse_cost(y, y_hat)\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc98289-cc92-48eb-998f-d8fc3c943f1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='4'></a>\n",
    "### Gradient Descent and Finding the Best Fit\n",
    "Now that we have a way of getting the derivative we need a way of reaching the best fit. This is done with an alogorithem called gradiant decent.\n",
    "\n",
    "Gradiant decent is an algorithem that pushes the values of $w, b$ in the right direction.\n",
    "___\n",
    "\n",
    "What this mean is that we go through a loop that runs a certain amount of times, and each time we calculate $\\frac{\\partial{J}}{\\partial{w}}, \\frac{\\partial{J}}{\\partial{b}}$. which we know are the slopes of the function $J(w, b)$ so if we think of the function $J$ as a hill, the negative of the slope tells us which way is down.\n",
    "___\n",
    "\n",
    "What gradiant decent does is push $w, b$ by $-\\alpha$ (aka: learning rate) times $\\frac{\\partial{J}}{\\partial{w}} , \\frac{\\partial{J}}{\\partial{b}}$. or in other words we push $w, b$ down hill to reach best fit. \n",
    "> In each loop we do the folloing operation - \n",
    "\n",
    "$$\n",
    "b = b -\\alpha\\left(\\frac{\\partial{J}}{\\partial{b}}\\right) \\quad w = w -\\alpha\\left(\\frac{\\partial{J}}{\\partial{w}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8911ac23-7859-45d5-811d-a88f9ace261e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradiant_decent(w, b, X, y, learning_rate=0.001, epochs=1000):\n",
    "    ''' Rreturns the optimal patameters for w and b and a dictionary conatining the changing cost per 100 loops.'''\n",
    "    costs_dict = {}\n",
    "    for i in range(epochs):\n",
    "        grads, cost = compute_gradiants(w, b, X, y)\n",
    "        w -= learning_rate * grads['dw']\n",
    "        b -= learning_rate * grads['db']\n",
    "        \n",
    "        if 100 % i == 0:\n",
    "            costs_dict[i] = cost \n",
    "            \n",
    "    return w, b, costs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d23e421-3cea-45e1-81d4-ee7443e756d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='5'></a>\n",
    "### Crating the Model\n",
    "Now we have every thing we need to create and train the linear regreesion model. \n",
    "> No lets create a class to better track the training process, and see the model in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c89ca146-6472-4263-89d9-f83510e5e7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "        self.features = self.X_train.shape[1]\n",
    "        self.w, self.b = generate_params(features)\n",
    "        \n",
    "    def fit(self, learning_rate=0.001, epochs=1000):\n",
    "        w, b, costs_dict = gradiant_decent(self.w, self.b, self.X_train, self.y_train, learning_rate, epochs)\n",
    "        self.costs = costs\n",
    "        \n",
    "    def plot_learning_curve(self, costs_dict):\n",
    "        \n",
    "        items = costs_dict.items()\n",
    "        iterations = [item[0] for item in items]\n",
    "        cost = [item[1] for item in items]\n",
    "        \n",
    "        plt.plot(iterations, cost)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
