{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T19:30:59.513607Z",
     "iopub.status.busy": "2025-06-02T19:30:59.513271Z",
     "iopub.status.idle": "2025-06-02T19:30:59.519561Z",
     "shell.execute_reply": "2025-06-02T19:30:59.518696Z",
     "shell.execute_reply.started": "2025-06-02T19:30:59.513585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "plt.style.use('bmh')\n",
    "sns.set_context('notebook')\n",
    "set_matplotlib_formats('retina')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-02T19:31:00.074860Z",
     "iopub.status.busy": "2025-06-02T19:31:00.074579Z",
     "iopub.status.idle": "2025-06-02T19:31:00.083624Z",
     "shell.execute_reply": "2025-06-02T19:31:00.082634Z",
     "shell.execute_reply.started": "2025-06-02T19:31:00.074841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_path = 'spaceship-titanic/'\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(dataset_path):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T19:31:00.785276Z",
     "iopub.status.busy": "2025-06-02T19:31:00.785001Z",
     "iopub.status.idle": "2025-06-02T19:31:00.834918Z",
     "shell.execute_reply": "2025-06-02T19:31:00.833786Z",
     "shell.execute_reply.started": "2025-06-02T19:31:00.785259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(dataset_path + 'train.csv')\n",
    "test_df = pd.read_csv(dataset_path + 'test.csv')\n",
    "print(f'Train: {train_df.shape}, Test: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T19:31:05.768648Z",
     "iopub.status.busy": "2025-06-02T19:31:05.768322Z",
     "iopub.status.idle": "2025-06-02T19:31:05.788748Z",
     "shell.execute_reply": "2025-06-02T19:31:05.787571Z",
     "shell.execute_reply.started": "2025-06-02T19:31:05.768625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T19:40:42.210465Z",
     "iopub.status.busy": "2025-06-02T19:40:42.210160Z",
     "iopub.status.idle": "2025-06-02T19:40:42.247043Z",
     "shell.execute_reply": "2025-06-02T19:40:42.245771Z",
     "shell.execute_reply.started": "2025-06-02T19:40:42.210445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def info(df):\n",
    "    info_df = {\n",
    "        'dtype': df.dtypes,\n",
    "        'nunique': df.nunique(),\n",
    "        'nulls': df.isna().sum(axis=0)\n",
    "    }\n",
    "    return pd.DataFrame(info_df)\n",
    "\n",
    "def nulls_df(train_df, test_df):\n",
    "    return pd.DataFrame(\n",
    "        columns=['Train Nulls', 'Test Nulls'], \n",
    "        data=zip(train_df.drop('Transported', axis=1).isna().sum(axis=0), test_df.isna().sum(axis=0)),\n",
    "        index=test_df.columns\n",
    "    )\n",
    "\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spending_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "train_df[spending_cols] = train_df[spending_cols].fillna(0)\n",
    "train_df['TotalBill'] = train_df[spending_cols].sum(axis=1)\n",
    "\n",
    "test_df[spending_cols] = test_df[spending_cols].fillna(0)\n",
    "test_df['TotalBill'] = test_df[spending_cols].sum(axis=1)\n",
    "\n",
    "nulls_df(train_df, test_df).loc[spending_cols].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_for_plot = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'TotalBill']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(11.5, 5.5))\n",
    "for ax, col in zip(axes.flatten(), cols_for_plot):\n",
    "    sns.scatterplot(ax=ax, data=train_df[col], linewidth=0, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[cols_for_plot] = np.log1p(train_df[cols_for_plot])\n",
    "cols_for_plot = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'TotalBill']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(11.5, 5.5))\n",
    "for ax, col in zip(axes.flatten(), cols_for_plot):\n",
    "    sns.scatterplot(ax=ax, data=train_df[col], linewidth=0, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['LastName'] = train_df['Name'].str.extract(r'([a-zA-Z]\\w+$)')\n",
    "# test_df['LastName'] = test_df['Name'].str.extract(r'([a-zA-Z]\\w+$)')\n",
    "# | Extracted LastName and Found {test_df['LastName'].nunique()} Unique Last Names\n",
    "\n",
    "train_df.drop('Name', axis=1, inplace=True)\n",
    "test_df.drop('Name', axis=1, inplace=True)\n",
    "print(f\"\"\"Name Column Manipulation:\n",
    "| Dropped the Original Name Column\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Age'] = train_df['Age'].fillna(train_df['Age'].median())\n",
    "test_df['Age'] = test_df['Age'].fillna(test_df['Age'].median())\n",
    "\n",
    "train_df['CryoSleep'] = train_df['CryoSleep'].fillna(train_df['CryoSleep'].mode()[0]).astype(int)\n",
    "test_df['CryoSleep'] = test_df['CryoSleep'].fillna(test_df['CryoSleep'].mode()[0]).astype(int)\n",
    "\n",
    "train_df['VIP'] = train_df['VIP'].fillna(train_df['VIP'].mode()[0]).astype(int)\n",
    "test_df['VIP'] = test_df['VIP'].fillna(test_df['VIP'].mode()[0]).astype(int)\n",
    "\n",
    "train_df['HomePlanet'] = train_df['HomePlanet'].fillna(train_df['HomePlanet'].mode()[0])\n",
    "test_df['HomePlanet'] = test_df['HomePlanet'].fillna(test_df['HomePlanet'].mode()[0])\n",
    "\n",
    "train_df['Destination'] = train_df['Destination'].fillna(train_df['Destination'].mode()[0])\n",
    "test_df['Destination'] = test_df['Destination'].fillna(test_df['Destination'].mode()[0])\n",
    "\n",
    "print(f\"\"\"Filled missing values:\n",
    "| Age: median -> {train_df['Age'].median()}\n",
    "| CryoSleep: mode -> {train_df['CryoSleep'].mode()[0]}, cast to int\n",
    "| VIP: mode -> {train_df['VIP'].mode()[0]}, cast to int\n",
    "| HomePlanet: mode -> {train_df['HomePlanet'].mode()[0]}\n",
    "| Destination: mode -> {train_df['Destination'].mode()[0]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['HasCabin'] = train_df['Cabin'].notna().astype(int)\n",
    "train_df['Deck'] = train_df['Cabin'].str.extract(r'(\\w)').fillna('U')\n",
    "train_df['CabinNumber'] = train_df['Cabin'].str.extract(r'(\\d)').fillna(-1).astype(int)\n",
    "train_df['Side'] =  train_df['Cabin'].str.extract(r'(\\w$)').fillna('U')\n",
    "train_df.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "test_df['HasCabin'] = test_df['Cabin'].notna().astype(int)\n",
    "test_df['Deck'] = test_df['Cabin'].str.extract(r'(\\w)').fillna('U')\n",
    "test_df['CabinNumber'] = test_df['Cabin'].str.extract(r'(\\d)').fillna(-1).astype(int)\n",
    "test_df['Side'] =  test_df['Cabin'].str.extract(r'(\\w$)').fillna('U')\n",
    "test_df.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "print(f\"\"\"Extracted cabin features:\n",
    "| HasCabin created: {train_df['HasCabin'].sum()} passengers had cabin info\n",
    "| Deck: missing filled with 'U'\n",
    "| CabinNumber: {train_df['CabinNumber'].nunique()} unique values, missing filled with -1\n",
    "| Side: missing filled with 'U'\n",
    "| Dropped original Cabin column\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck_mean_transport = train_df \\\n",
    "    .groupby('Deck')['Transported']\\\n",
    "    .mean() \\\n",
    "    .sort_values(ascending=False) \\\n",
    "    .to_frame().T\n",
    "\n",
    "fig = plt.figure(figsize=(9, 4))\n",
    "sns.barplot(deck_mean_transport, edgecolor='black', zorder=3, alpha=0.8)\n",
    "plt.title('Chance of Transport for each Deck')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['PassengerGroup'] = train_df['PassengerId'].str.extract(r'([0-9]*(?=_))').astype(int)\n",
    "train_df['GroupSize'] = train_df.groupby('PassengerGroup')['PassengerId'].transform('count')\n",
    "\n",
    "test_df['PassengerGroup'] = test_df['PassengerId'].str.extract(r'([0-9]*(?=_))').astype(int)\n",
    "test_df['GroupSize'] = test_df.groupby('PassengerGroup')['PassengerId'].transform('count')\n",
    "\n",
    "print(f\"\"\"\n",
    "Extracted PassengerGroup from PassengerId and calculated group sizes:\n",
    "- Total unique groups in train: {train_df['PassengerGroup'].nunique()}\n",
    "- Total unique groups in test: {test_df['PassengerGroup'].nunique()}\n",
    "- Avg group size in train: {train_df['GroupSize'].mean():.2f}\n",
    "- Avg group size in test: {test_df['GroupSize'].mean():.2f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.get_dummies(\n",
    "    train_df,\n",
    "    dtype=int,\n",
    "    columns=['Deck', 'Side', 'Destination', 'HomePlanet'],\n",
    "    prefix_sep={'Deck': '-', 'Side': '-', 'Destination': '-', 'HomePlanet': '-'}\n",
    "    )\n",
    "\n",
    "test_df = pd.get_dummies(\n",
    "    test_df,\n",
    "    dtype=int,\n",
    "    columns=['Deck', 'Side', 'Destination', 'HomePlanet'],\n",
    "    prefix_sep={'Deck': '-', 'Side': '-', 'Destination': '-', 'HomePlanet': '-'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictors = train_df.drop(['PassengerId', 'Transported'], axis=1).to_numpy()\n",
    "test_predictors = test_df.drop('PassengerId', axis=1).to_numpy()\n",
    "train_target = train_df['Transported'].to_numpy()\n",
    "\n",
    "print(f\"\"\"\n",
    "X.shape: {train_predictors.shape}, X_test.shape: {test_predictors.shape}\n",
    "y.shape: {train_target.shape}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler, \n",
    "    'Normalizer': Normalizer, \n",
    "    'MinMaxScaler': MinMaxScaler, \n",
    "    'RobustScaler': RobustScaler\n",
    "}\n",
    "for name, feature_scaler in scalers.items():\n",
    "\n",
    "    scelar_selector_pipe = Pipeline([\n",
    "        ('scaler', feature_scaler()),\n",
    "        ('model', LogisticRegression(random_state=SEED))\n",
    "    ]) \\\n",
    "    .fit(train_predictors, train_target)\n",
    "\n",
    "    preds = scelar_selector_pipe.predict(train_predictors)\n",
    "    acc = accuracy_score(train_target, preds)\n",
    "    prec = precision_score(train_target, preds)\n",
    "    recl = recall_score(train_target, preds)\n",
    "    f1 = f1_score(train_target, preds)\n",
    "\n",
    "    print(f\"\"\"| {name:<14} | Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {recl:.4f} | F1: {f1:.4f} |\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LearningCurveDisplay, learning_curve\n",
    "\n",
    "# plot learning curves for different train subset choise\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 6.5))\n",
    "for ax, scaler in zip(axes.flatten(), scalers):\n",
    "    test_pipe = Pipeline([\n",
    "            ('scaler', scalers[scaler]()),\n",
    "            ('model', LogisticRegression(random_state=SEED))\n",
    "        ]) \\\n",
    "        .fit(train_predictors, train_target)\n",
    "\n",
    "    train_size_abs, train_scores, test_scores = learning_curve(scelar_selector_pipe, train_predictors, train_target)\n",
    "    display = LearningCurveDisplay(\n",
    "        train_sizes=train_size_abs, \n",
    "        train_scores=train_scores, \n",
    "        test_scores=test_scores, \n",
    "        score_name=\"Score\"\n",
    "        )\n",
    "    display.plot(ax=ax)\n",
    "    ax.set_title(scaler, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_predictors, train_target, test_size=.15, shuffle=True)\n",
    "print(f\"\"\"\n",
    "| X_train.shape: {X_train.shape} | X_val.shape: {X_val.shape} | X_test.shape: {test_predictors.shape}\n",
    "| -----------------------------------------------------------------------------\n",
    "| y_train.shape: {y_train.shape} | y_val.shape: {y_val.shape}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_scaler = RobustScaler\n",
    "pipe_model = LogisticRegression\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', pipe_scaler()),\n",
    "    ('model', pipe_model(random_state=SEED))\n",
    "])\\\n",
    "    .fit(X_train, y_train)\n",
    "\n",
    "preds = pipe.predict(X_val)\n",
    "acc = accuracy_score(y_val, preds)\n",
    "prec = precision_score(y_val, preds)\n",
    "recl = recall_score(y_val, preds)\n",
    "f1 = f1_score(y_val, preds)\n",
    "\n",
    "print(f\"\"\"| Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {recl:.4f} | F1: {f1:.4f} |\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pipe.predict(test_predictors)\n",
    "\n",
    "submission_tuple = zip(test_df['PassengerId'], y_test)\n",
    "submission = pd.DataFrame(submission_tuple, columns=['PassengerId', 'Transported'])\n",
    "submission.to_csv('predictions.csv', index=False)\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 3220602,
     "sourceId": 34377,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
